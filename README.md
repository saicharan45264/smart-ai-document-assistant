ğŸš€ Smart University Assistant

AI-powered document analysis, intelligent query answering, and academic support system built using FastAPI, Next.js, LangChain, ChromaDB, and OpenAI.

This project allows students & faculty to upload university handbooks, PDFs, notices, or academic documents and ask natural language questions, receiving accurate answers backed by retrieval-augmented generation (RAG).

â¸»

â­ Features
	â€¢	ğŸ“„ PDF & Document Uploading (supports PDFs, text files, etc.)
	â€¢	ğŸ§© Automatic Text Extraction using PyPDF
	â€¢	ğŸ” Semantic Search with Chroma Vector DB
	â€¢	ğŸ§  RAG Pipeline using LangChain
	â€¢	ğŸ¤– LLM-powered Chat Interface (OpenAI / Gemini / any LLM)
	â€¢	âš¡ Real-time Responses through FastAPI API
	â€¢	ğŸ’» Modern Frontend using Next.js + TailwindCSS
	â€¢	ğŸ—ï¸ Modular & Extensible Architecture

â¸»

ğŸ›ï¸ Tech Stack

Backend
	â€¢	FastAPI
	â€¢	LangChain
	â€¢	ChromaDB
	â€¢	PyPDF
	â€¢	OpenAI API
	â€¢	Uvicorn

Frontend
	â€¢	Next.js
	â€¢	React
	â€¢	Tailwind CSS


ğŸ§¬ Architecture Overview

User â†’ Next.js Frontend â†’ FastAPI Backend â†’ 
PDF Processing â†’ Chunking â†’ Embeddings â†’ ChromaDB â†’
RAG Pipeline â†’ LLM â†’ Response â†’ Frontend Chat UI


Flow
	1.	User uploads a document
	2.	Backend extracts text & chunks it
	3.	Embeddings generated via LangChain
	4.	Stored in Chroma vectorstore
	5.	User asks question
	6.	Relevant chunks retrieved
	7.	Sent to LLM for answer


âš™ï¸ Backend Setup

1. Create virtual environment:

python3 -m venv .venv
source .venv/bin/activate


2. Install dependencies:

pip install -r requirements.txt

3. Run backend

uvicorn app.main:app --reload

API will be available at: 
http://127.0.0.1:8000
http://127.0.0.1:8000/docs


ğŸ’» Frontend Setup:

1. Navigate to frontend: cd sua-frontend
2. 2. Install dependencies: npm install
3. npm run dev
Frontend will run at: http://localhost:3000



ğŸ“„ Project Structure:

smart-ai-document-assistant/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”‚   â”œâ”€â”€ upload.py
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ db/
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ sua-frontend/
â”‚   â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ styles/
â”‚   â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ next.config.js
â”‚
â””â”€â”€ README.md



ğŸ§ª API Endpoints

POST /upload

Upload PDF â†’ stores embeddings in Chroma.

POST /chat

Ask a question â†’ backend retrieves context â†’ LLM generates answer.

â¸»

ğŸ› ï¸ Customization

You can easily modify:
	â€¢	Embedding model
	â€¢	LLM model (OpenAI, Gemini, Ollama, Llama, etc.)
	â€¢	Chunk size
	â€¢	Prompt format
	â€¢	Retrieval parameters
	â€¢	Frontend UI

â¸»

ğŸ¯ Use Cases
	â€¢	University handbook search
	â€¢	Exam rule lookup
	â€¢	Course catalog Q&A
	â€¢	Department document search
	â€¢	Policy-based intelligent question answering

â¸»

ğŸ¤ Contributions

Pull requests and feature improvements are welcome.
